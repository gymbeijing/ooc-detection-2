{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01107538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/yg007/nytimes_project/venv_nyc/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as data\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd2c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lavis.models import model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93a7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_zoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea44d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf3abf",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b8267f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# setup device to use\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1bfa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vis_processors, txt_processors = load_model_and_preprocess(\n",
    "    name = \"clip_feature_extractor\", model_type=\"ViT-B-32\", is_eval=True, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70514066",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbdae0",
   "metadata": {},
   "source": [
    "# Define the Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d51bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterCOMMsDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): Path to the {train_completed|val_completed}.csv file.\n",
    "            image_folder_dir (string): Directory containing the images\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path, index_col=0)\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        self.df['exists'] = self.df['filename'].apply(lambda filename: os.path.exists(os.path.join(img_dir, filename)))\n",
    "        delete_row = self.df[self.df[\"exists\"]==False].index\n",
    "        self.df = self.df.drop(delete_row)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def remove_URL(self, text):\n",
    "        \"\"\"Remove URLs from a sample string\"\"\"\n",
    "        return re.sub(r\"http\\S+\", '', text)\n",
    "    \n",
    "    def remove_punc(self, text):\n",
    "        \"\"\"Remove punctuation from a sample string\"\"\"\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        \n",
    "        topic = item['topic']\n",
    "        falsified = int(item['falsified'])\n",
    "        not_falsified = float(not item['falsified'])\n",
    "#         label = np.array((falsified, not_falsified))\n",
    "        label = np.array(falsified)\n",
    "        domain = topic.split('_')[0]\n",
    "        diff = topic.split('_')[1]\n",
    "        \n",
    "        # Preprocess the text\n",
    "        caption = item['full_text']\n",
    "        caption = ' '.join(tt.tokenize(caption))\n",
    "        caption = self.remove_punc(self.remove_URL(caption))\n",
    "        \n",
    "        # Open the image\n",
    "        img_filename = item['filename']\n",
    "        raw_image = Image.open(os.path.join(self.img_dir, img_filename)).convert('RGB')\n",
    "        \n",
    "        # Get the multimodal embedding\n",
    "        image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "        text_input = txt_processors[\"eval\"](caption)\n",
    "        sample = {\"image\": image, \"text_input\": [text_input]}   # image shape: [1, 3, 224, 224]\n",
    "        features = model.extract_features(sample)\n",
    "        features_image = features.image_embeds   # [1, 512]\n",
    "        features_text = features.text_embeds   # [1, 512]\n",
    "#         multimodal_emb = torch.cat((features_image, features_text), 1)\n",
    "        multimodal_emb = features_image * features_text\n",
    "        \n",
    "        cos_sim = util.cos_sim(features_text, features_image)\n",
    "#         features_image_proj = features_image.image_embeds_proj[:,0,:]   # [1, 256]\n",
    "#         features_text_proj = features_text.text_embeds_proj[:,0,:]   # [1, 256]\n",
    "        \n",
    "#         multimodal_emb = torch.cat((features_image_proj, features_text_proj), 1)\n",
    "#         multimodal_emb = features_image_proj * features_text_proj   # [1, 256]\n",
    "#         print(multimodal_emb.shape)\n",
    "\n",
    "#         similarity = features_image_proj @ features_text_proj.t()\n",
    "\n",
    "        return {\"multimodal_emb\": multimodal_emb,\n",
    "                \"topic\": topic, \n",
    "                \"label\": label, \n",
    "                \"domain\": domain, \n",
    "                \"difficulty\": diff,\n",
    "               \"similarity\": cos_sim}\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c1898ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClimateAndCovidDataset(Dataset):\n",
    "#     def __init__(self, csv_path, img_dir):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             csv_path (string): Path to the {train_completed|val_completed}.csv file.\n",
    "#             image_folder_dir (string): Directory containing the images\n",
    "#         \"\"\"\n",
    "#         self.df = pd.read_csv(csv_path, index_col=0)\n",
    "#         self.img_dir = img_dir\n",
    "        \n",
    "#         self.df['exists'] = self.df['filename'].apply(lambda filename: os.path.exists(os.path.join(img_dir, filename)))\n",
    "#         delete_row = self.df[self.df[\"exists\"]==False].index\n",
    "#         self.df = self.df.drop(delete_row)\n",
    "        \n",
    "#         self.df['is_military'] = self.df['topic'].apply(lambda topic: 'military' in topic)\n",
    "#         delete_row = self.df[self.df[\"is_military\"]==True].index\n",
    "#         self.df = self.df.drop(delete_row)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.df.iloc[idx]\n",
    "#         caption = item['full_text']\n",
    "#         img_filename = item['filename']\n",
    "#         topic = item['topic']\n",
    "#         falsified = float(item['falsified'])\n",
    "#         not_falsified = float(not item['falsified'])\n",
    "#         label = np.array((falsified, not_falsified))\n",
    "#         domain = topic.split('_')[0]\n",
    "#         diff = topic.split('_')[1]\n",
    "        \n",
    "#         try:\n",
    "#             raw_image = Image.open(os.path.join(self.img_dir, img_filename)).convert('RGB')\n",
    "#             image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "#             text_input = txt_processors[\"eval\"](caption)\n",
    "#             sample = {\"image\": image, \"text_input\": [text_input]}   # image shape: [1, 3, 224, 224]\n",
    "        \n",
    "#             features_multimodal = model.extract_features(sample, mode=\"multimodal\")\n",
    "# #             features_image = model.extract_features(sample, mode=\"image\")\n",
    "# #             features_text = model.extract_features(sample, mode=\"text\")\n",
    "# #             print(features_multimodal.multimodal_embeds[:, 0, :].shape)\n",
    "        \n",
    "#             return {\"multimodal_emb\": features_multimodal.multimodal_embeds[:, 0, :],\n",
    "#                     \"topic\": topic, \n",
    "#                     \"label\": label, \n",
    "#                     \"domain\": domain, \n",
    "#                     \"difficulty\": diff}\n",
    "        \n",
    "#         except IOError as e:\n",
    "#             print(e)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f91a7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = TwitterCOMMsDataset(csv_path='../data/train_completed.csv',\n",
    "#                                     img_dir='/import/network-temp/yimengg/data/twitter-comms/train/images/train_image_ids')   # took ~one hour to construct the dataset\n",
    "val_data = TwitterCOMMsDataset(csv_path='../data/val_completed.csv', \n",
    "                               img_dir='/import/network-temp/yimengg/data/twitter-comms/images/val_images/val_tweet_image_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b576b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]['multimodal_emb'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2306202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a243713",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# train_iterator = data.DataLoader(train_data, \n",
    "#                                  shuffle = True, \n",
    "#                                  batch_size=BATCH_SIZE)\n",
    "val_iterator = data.DataLoader(val_data, \n",
    "                               shuffle = False, \n",
    "                               batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eb021b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=2):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(in_dim, out_dim)\n",
    "        self.in_dim = in_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.in_dim)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "    \n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11557e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e5f1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    net = Net(512)\n",
    "#     net = Net(1024)\n",
    "    net.cuda()\n",
    "    net.train()\n",
    "    net.weight_init(mean=0, std=0.02)\n",
    "    \n",
    "    lr = 0.0001\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(device)\n",
    "    \n",
    "    softmax = nn.Softmax(dim=1)\n",
    " \n",
    "    EPOCHS = 2\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        num_correct = 0\n",
    "        total = 0\n",
    "        for i, batch in tqdm(enumerate(val_iterator, 0), desc='iterations'):\n",
    "            inputs = batch[\"multimodal_emb\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            net.zero_grad()\n",
    "            y_preds = net(inputs)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "#             _, top_pred = y_preds.topk(1, 1)\n",
    "            \n",
    "            top_pred = torch.zeros_like(labels)\n",
    "            y_preds = softmax(y_preds)\n",
    "#             print(y_preds[:, 0])\n",
    "            top_pred[y_preds[:, 1] >= 0.5] = 1\n",
    "            y = labels.cpu()\n",
    "            batch_size = y.shape[0]\n",
    "            top_pred = top_pred.cpu().view(batch_size)\n",
    "            \n",
    "#             num_correct += sum(top_pred == y[:, 0]).item()\n",
    "            num_correct += sum(top_pred == y).item()\n",
    "            total += batch_size\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Epoch [%d/%d]: Training accuracy %.2f, training loss %.3f\" % (epoch+1, EPOCHS, num_correct/total, total_loss/total))\n",
    "                print(y_preds[:, 1])\n",
    "#                 print(top_pred)\n",
    "#                 print(labels)\n",
    "#                 print(num_correct)\n",
    "#                 print(total)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "442294f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 1it [00:01,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.56, training loss 0.022\n",
      "tensor([0.4662, 0.4662, 0.4780, 0.4651, 0.4848, 0.4562, 0.4837, 0.4837, 0.4893,\n",
      "        0.4970, 0.4970, 0.4867, 0.4793, 0.4793, 0.4764, 0.4680, 0.4674, 0.4655,\n",
      "        0.4655, 0.4979, 0.5008, 0.5283, 0.5283, 0.5076, 0.4800, 0.4800, 0.4818,\n",
      "        0.5099, 0.5035, 0.4918, 0.4918, 0.4911], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 51it [01:49,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.53, training loss 0.022\n",
      "tensor([0.4662, 0.4916, 0.4608, 0.5038, 0.5038, 0.5075, 0.5075, 0.4882, 0.5114,\n",
      "        0.4845, 0.4845, 0.4939, 0.5137, 0.4949, 0.4949, 0.4915, 0.4845, 0.4845,\n",
      "        0.4754, 0.4639, 0.4498, 0.4498, 0.4770, 0.4947, 0.4947, 0.4901, 0.4670,\n",
      "        0.5153, 0.4863, 0.4863, 0.5044, 0.4761], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 101it [03:21,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.53, training loss 0.022\n",
      "tensor([0.4479, 0.4690, 0.4640, 0.4640, 0.4708, 0.4545, 0.4382, 0.4382, 0.4401,\n",
      "        0.4401, 0.4997, 0.4889, 0.4564, 0.4693, 0.4693, 0.4754, 0.4646, 0.4646,\n",
      "        0.4780, 0.4667, 0.4940, 0.4536, 0.4536, 0.4514, 0.4676, 0.4676, 0.4763,\n",
      "        0.5014, 0.4794, 0.4978, 0.4814, 0.4814], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 151it [05:05,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.55, training loss 0.021\n",
      "tensor([0.5078, 0.4945, 0.4693, 0.4644, 0.4644, 0.4923, 0.4732, 0.4451, 0.4451,\n",
      "        0.4605, 0.4680, 0.4880, 0.4880, 0.4938, 0.4813, 0.4554, 0.4554, 0.4633,\n",
      "        0.4809, 0.4744, 0.4612, 0.4612, 0.4164, 0.4164, 0.5066, 0.4826, 0.4629,\n",
      "        0.4251, 0.4251, 0.4868, 0.4771, 0.4771], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 201it [06:41,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.56, training loss 0.021\n",
      "tensor([0.4494, 0.4494, 0.4583, 0.4583, 0.5114, 0.4812, 0.4774, 0.4774, 0.4903,\n",
      "        0.5156, 0.5000, 0.4901, 0.4901, 0.5071, 0.4839, 0.4560, 0.4560, 0.4451,\n",
      "        0.4451, 0.4567, 0.4574, 0.4581, 0.4633, 0.4633, 0.4895, 0.4947, 0.4299,\n",
      "        0.4299, 0.4604, 0.4988, 0.4739, 0.4739], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 251it [08:23,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.57, training loss 0.021\n",
      "tensor([0.4606, 0.4606, 0.4558, 0.4858, 0.4689, 0.4761, 0.4246, 0.4246, 0.5073,\n",
      "        0.4572, 0.4572, 0.5005, 0.5072, 0.4997, 0.4997, 0.4672, 0.4906, 0.4816,\n",
      "        0.4816, 0.4599, 0.4509, 0.4509, 0.4520, 0.5154, 0.4548, 0.4548, 0.4912,\n",
      "        0.4323, 0.4856, 0.4466, 0.4466, 0.4635], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 301it [09:47,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.58, training loss 0.021\n",
      "tensor([0.4218, 0.4218, 0.4814, 0.4596, 0.4120, 0.4200, 0.4200, 0.4898, 0.5205,\n",
      "        0.5205, 0.4628, 0.4628, 0.5096, 0.5129, 0.5236, 0.4144, 0.4189, 0.4189,\n",
      "        0.4380, 0.4380, 0.4931, 0.4699, 0.4514, 0.4514, 0.4523, 0.4523, 0.4823,\n",
      "        0.5074, 0.4614, 0.4220, 0.4220, 0.4760], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 351it [11:17,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.58, training loss 0.021\n",
      "tensor([0.4208, 0.4198, 0.4512, 0.4512, 0.4573, 0.4386, 0.4162, 0.4162, 0.4472,\n",
      "        0.4472, 0.4530, 0.4160, 0.4535, 0.4535, 0.4571, 0.4957, 0.4712, 0.4744,\n",
      "        0.4744, 0.3939, 0.3939, 0.4618, 0.4642, 0.4365, 0.4365, 0.5093, 0.4766,\n",
      "        0.4766, 0.5004, 0.4118, 0.4563, 0.3756], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 401it [12:55,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.59, training loss 0.021\n",
      "tensor([0.4840, 0.4896, 0.4822, 0.4596, 0.4596, 0.4541, 0.4833, 0.3999, 0.3999,\n",
      "        0.4426, 0.4845, 0.4382, 0.4382, 0.4499, 0.4499, 0.5082, 0.4643, 0.4643,\n",
      "        0.4596, 0.3824, 0.3824, 0.4528, 0.5053, 0.4837, 0.5039, 0.4449, 0.4449,\n",
      "        0.4059, 0.4059, 0.4324, 0.4082, 0.4972], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 451it [14:23,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.60, training loss 0.021\n",
      "tensor([0.4339, 0.4339, 0.3987, 0.3987, 0.4917, 0.4748, 0.3872, 0.3462, 0.3462,\n",
      "        0.3746, 0.4325, 0.5244, 0.4355, 0.4355, 0.4421, 0.4421, 0.4799, 0.5147,\n",
      "        0.5147, 0.5129, 0.4937, 0.3358, 0.3358, 0.4361, 0.4863, 0.4305, 0.4305,\n",
      "        0.4221, 0.4221, 0.4648, 0.4053, 0.3405], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 501it [15:56,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.60, training loss 0.021\n",
      "tensor([0.4076, 0.4523, 0.3927, 0.3927, 0.4034, 0.5016, 0.4214, 0.4214, 0.4875,\n",
      "        0.3766, 0.3766, 0.4209, 0.3476, 0.3896, 0.4508, 0.3765, 0.3765, 0.4440,\n",
      "        0.4056, 0.4109, 0.4109, 0.4511, 0.4135, 0.4135, 0.4564, 0.4723, 0.4202,\n",
      "        0.4312, 0.4312, 0.3843, 0.3843, 0.4219], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 551it [17:24,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.59, training loss 0.021\n",
      "tensor([0.3716, 0.4909, 0.5549, 0.4794, 0.4728, 0.4728, 0.5374, 0.4578, 0.4238,\n",
      "        0.4238, 0.4516, 0.4516, 0.4418, 0.4746, 0.4273, 0.4711, 0.4180, 0.4180,\n",
      "        0.5104, 0.3748, 0.4139, 0.4139, 0.4465, 0.4465, 0.4172, 0.4431, 0.4388,\n",
      "        0.4872, 0.4468, 0.4468, 0.5254, 0.4179], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 601it [18:53,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.60, training loss 0.021\n",
      "tensor([0.3445, 0.4939, 0.3580, 0.4213, 0.4213, 0.5029, 0.4632, 0.4583, 0.3926,\n",
      "        0.4110, 0.4110, 0.5265, 0.4903, 0.4903, 0.4552, 0.4707, 0.4707, 0.4507,\n",
      "        0.4312, 0.4456, 0.3539, 0.3539, 0.4511, 0.4061, 0.4061, 0.3980, 0.4777,\n",
      "        0.4236, 0.4236, 0.4575, 0.4224, 0.4982], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 651it [20:32,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: Training accuracy 0.60, training loss 0.021\n",
      "tensor([0.4804, 0.4804, 0.4392, 0.4382, 0.4382, 0.4480, 0.4543, 0.4543, 0.5195,\n",
      "        0.4624, 0.4139, 0.4139, 0.4930, 0.4316, 0.4720, 0.3766, 0.3766, 0.5009,\n",
      "        0.5009, 0.5251, 0.5650, 0.4631, 0.4631, 0.4646, 0.5406, 0.4635, 0.4377,\n",
      "        0.4377, 0.5252, 0.5252, 0.5340, 0.5196], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 691it [21:41,  1.88s/it]\n",
      "iterations: 1it [00:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.72, training loss 0.020\n",
      "tensor([0.4070, 0.4070, 0.4644, 0.5267, 0.4773, 0.4849, 0.4427, 0.4427, 0.5522,\n",
      "        0.3656, 0.3656, 0.5475, 0.5829, 0.5829, 0.6039, 0.5547, 0.5023, 0.4535,\n",
      "        0.4535, 0.6189, 0.4890, 0.4773, 0.4773, 0.4901, 0.3882, 0.3882, 0.5019,\n",
      "        0.5513, 0.5502, 0.5446, 0.5446, 0.5802], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 51it [01:45,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.70, training loss 0.020\n",
      "tensor([0.4423, 0.4458, 0.4483, 0.4666, 0.4666, 0.4117, 0.4117, 0.6216, 0.4790,\n",
      "        0.4087, 0.4087, 0.5700, 0.5511, 0.3758, 0.3758, 0.5516, 0.4504, 0.4504,\n",
      "        0.4359, 0.4517, 0.4546, 0.4546, 0.5333, 0.4916, 0.4916, 0.5440, 0.5265,\n",
      "        0.4949, 0.3825, 0.3825, 0.5206, 0.5968], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 101it [03:15,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.70, training loss 0.020\n",
      "tensor([0.4353, 0.5096, 0.4796, 0.4796, 0.5669, 0.4779, 0.3680, 0.3680, 0.4152,\n",
      "        0.4152, 0.5693, 0.5739, 0.4732, 0.4886, 0.4886, 0.5524, 0.4835, 0.4835,\n",
      "        0.5798, 0.5349, 0.6425, 0.3831, 0.3831, 0.4456, 0.5190, 0.5190, 0.5569,\n",
      "        0.5617, 0.4897, 0.5521, 0.3594, 0.3594], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 151it [04:58,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.70, training loss 0.020\n",
      "tensor([0.5066, 0.4697, 0.5177, 0.4148, 0.4148, 0.5479, 0.4923, 0.3651, 0.3651,\n",
      "        0.3865, 0.5213, 0.5311, 0.5311, 0.5833, 0.5425, 0.5106, 0.5106, 0.5421,\n",
      "        0.4506, 0.4223, 0.4325, 0.4325, 0.3142, 0.3142, 0.5928, 0.5172, 0.4119,\n",
      "        0.3005, 0.3005, 0.5317, 0.4375, 0.4375], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 201it [06:34,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.71, training loss 0.020\n",
      "tensor([0.3656, 0.3656, 0.3717, 0.3717, 0.5463, 0.4518, 0.5568, 0.5568, 0.6029,\n",
      "        0.6250, 0.5200, 0.5408, 0.5408, 0.6119, 0.4506, 0.3822, 0.3822, 0.3601,\n",
      "        0.3601, 0.4734, 0.4541, 0.3761, 0.3629, 0.3629, 0.5203, 0.4682, 0.3165,\n",
      "        0.3165, 0.4158, 0.5370, 0.4784, 0.4784], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 251it [08:16,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.71, training loss 0.019\n",
      "tensor([0.4489, 0.4489, 0.4446, 0.5545, 0.4800, 0.5082, 0.3718, 0.3718, 0.5753,\n",
      "        0.4407, 0.4407, 0.5268, 0.5110, 0.5644, 0.5644, 0.4816, 0.5391, 0.4492,\n",
      "        0.4492, 0.4537, 0.3469, 0.3469, 0.4152, 0.6200, 0.4667, 0.4667, 0.5513,\n",
      "        0.4061, 0.5408, 0.4337, 0.4337, 0.4308], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 301it [09:41,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.71, training loss 0.019\n",
      "tensor([0.3416, 0.3416, 0.5788, 0.4418, 0.3367, 0.3279, 0.3279, 0.5567, 0.6204,\n",
      "        0.6204, 0.4283, 0.4283, 0.5696, 0.5864, 0.5880, 0.3567, 0.3618, 0.3618,\n",
      "        0.3995, 0.3995, 0.5406, 0.5076, 0.4407, 0.4407, 0.4353, 0.4353, 0.4930,\n",
      "        0.5642, 0.4989, 0.4208, 0.4208, 0.5367], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 351it [11:14,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.71, training loss 0.019\n",
      "tensor([0.3780, 0.3560, 0.4274, 0.4274, 0.5132, 0.3871, 0.3949, 0.3949, 0.4931,\n",
      "        0.4931, 0.4607, 0.3628, 0.4528, 0.4528, 0.4567, 0.5709, 0.5131, 0.5247,\n",
      "        0.5247, 0.2515, 0.2515, 0.4115, 0.4327, 0.3895, 0.3895, 0.5579, 0.5413,\n",
      "        0.5413, 0.5575, 0.3871, 0.5051, 0.2752], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 401it [12:58,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.71, training loss 0.019\n",
      "tensor([0.5506, 0.5249, 0.5258, 0.4674, 0.4674, 0.4713, 0.5260, 0.2981, 0.2981,\n",
      "        0.4074, 0.5400, 0.3839, 0.3839, 0.4523, 0.4523, 0.5877, 0.4657, 0.4657,\n",
      "        0.4794, 0.3133, 0.3133, 0.4846, 0.5594, 0.5410, 0.5438, 0.4435, 0.4435,\n",
      "        0.3825, 0.3825, 0.4103, 0.3730, 0.5429], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 451it [14:30,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.71, training loss 0.019\n",
      "tensor([0.4171, 0.4171, 0.3187, 0.3187, 0.5059, 0.4730, 0.3516, 0.2661, 0.2661,\n",
      "        0.3397, 0.4535, 0.5815, 0.4665, 0.4665, 0.4851, 0.4851, 0.5783, 0.5946,\n",
      "        0.5946, 0.5915, 0.5261, 0.2397, 0.2397, 0.4418, 0.5168, 0.4177, 0.4177,\n",
      "        0.4173, 0.4173, 0.4906, 0.4090, 0.3217], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 501it [16:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.71, training loss 0.019\n",
      "tensor([0.4104, 0.5076, 0.3763, 0.3763, 0.3924, 0.5733, 0.3611, 0.3611, 0.5438,\n",
      "        0.3423, 0.3423, 0.4490, 0.2999, 0.3592, 0.5075, 0.3335, 0.3335, 0.4843,\n",
      "        0.3932, 0.3924, 0.3924, 0.4829, 0.4265, 0.4265, 0.4831, 0.5201, 0.4124,\n",
      "        0.4120, 0.4120, 0.3532, 0.3532, 0.4072], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 551it [17:37,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.71, training loss 0.019\n",
      "tensor([0.2901, 0.4953, 0.6285, 0.4962, 0.4824, 0.4824, 0.5863, 0.4829, 0.4042,\n",
      "        0.4042, 0.4739, 0.4739, 0.4377, 0.5017, 0.4462, 0.4787, 0.4170, 0.4170,\n",
      "        0.5552, 0.3231, 0.3853, 0.3853, 0.4428, 0.4428, 0.4004, 0.4252, 0.4192,\n",
      "        0.4981, 0.4361, 0.4361, 0.6070, 0.4165], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 601it [19:07,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.70, training loss 0.019\n",
      "tensor([0.2586, 0.5460, 0.2935, 0.3680, 0.3680, 0.5789, 0.4646, 0.4564, 0.3394,\n",
      "        0.3702, 0.3702, 0.5956, 0.5500, 0.5500, 0.4736, 0.4809, 0.4809, 0.4583,\n",
      "        0.4248, 0.4585, 0.3036, 0.3036, 0.4746, 0.3671, 0.3671, 0.3572, 0.5105,\n",
      "        0.3937, 0.3937, 0.4639, 0.4014, 0.5345], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 651it [20:45,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: Training accuracy 0.70, training loss 0.019\n",
      "tensor([0.4919, 0.4919, 0.4307, 0.4311, 0.4311, 0.4667, 0.4553, 0.4553, 0.5320,\n",
      "        0.4723, 0.3911, 0.3911, 0.5026, 0.4012, 0.4961, 0.3398, 0.3398, 0.4972,\n",
      "        0.4972, 0.5515, 0.6199, 0.4279, 0.4279, 0.4519, 0.6008, 0.4721, 0.4071,\n",
      "        0.4071, 0.5791, 0.5791, 0.5849, 0.5849], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 691it [21:54,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "net = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e41e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5818ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 1it [00:01,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.50\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([0.3349, 0.3349, 0.2642, 0.1821, 0.2555, 0.2151, 0.3025, 0.3025, 0.2159,\n",
      "        0.3478, 0.3478, 0.2324, 0.2302, 0.2302, 0.2318, 0.2411, 0.2512, 0.2805,\n",
      "        0.2805, 0.1719, 0.2577, 0.2763, 0.2763, 0.2854, 0.3493, 0.3493, 0.2674,\n",
      "        0.1792, 0.1598, 0.2364, 0.2364, 0.1451], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 51it [01:48,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.49\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([0.2013, 0.2949, 0.2541, 0.2889, 0.2889, 0.3252, 0.3252, 0.1864, 0.2455,\n",
      "        0.3206, 0.3206, 0.2248, 0.1014, 0.3531, 0.3531, 0.2052, 0.2658, 0.2658,\n",
      "        0.2847, 0.2716, 0.2694, 0.2694, 0.1450, 0.2631, 0.2631, 0.1578, 0.2307,\n",
      "        0.2952, 0.3938, 0.3938, 0.2566, 0.2436], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterations: 57it [02:00,  2.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_152573/721078192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nytimes_project/venv_nyc/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nytimes_project/venv_nyc/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nytimes_project/venv_nyc/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nytimes_project/venv_nyc/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nytimes_project/venv_nyc/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_152573/1336602713.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Open the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mimg_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mraw_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Get the multimodal embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nytimes_project/venv_nyc/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transparency\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nytimes_project/venv_nyc/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "total = 0\n",
    "for i, batch in tqdm(enumerate(val_iterator, 0), desc='iterations'):\n",
    "    similarity = batch[\"similarity\"].squeeze()\n",
    "    labels = batch[\"label\"]\n",
    "    y_preds = torch.zeros_like(labels)\n",
    "#     y_preds[similarity < 0.25] = 1\n",
    "    y_preds[similarity < 0.5] = 1\n",
    "    \n",
    "    num_correct += sum(y_preds == labels).item()\n",
    "    total += BATCH_SIZE\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"Accuracy %.2f\" % (num_correct/total))\n",
    "        print(y_preds)\n",
    "        print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04932705",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('../data/val_completed.csv', index_col=0)\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118efe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/import/network-temp/yimengg/data/twitter-comms/images/val_images/val_tweet_image_ids'\n",
    "val_df['exists'] = val_df['filename'].apply(lambda filename: os.path.exists(os.path.join(img_dir, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_row = val_df[val_df[\"exists\"]==False].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df.drop(delete_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ba98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", '', text)\n",
    "\n",
    "def remove_punc(text):\n",
    "    \"\"\"Remove punctuation from a sample string\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "s = val_data.df.iloc[0]['full_text']\n",
    "print(s)\n",
    "s = ' '.join(tt.tokenize(s))\n",
    "print(remove_punc(remove_URL(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396aecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "print(val_data.df.iloc[idx]['full_text'])\n",
    "print(\"Falsified? \" + str(val_data.df.iloc[idx]['falsified']))\n",
    "print(val_data.df.iloc[idx]['topic'])\n",
    "print(val_data[idx]['similarity'])\n",
    "\n",
    "raw_image = Image.open('/import/network-temp/yimengg/data/twitter-comms/images/val_images/val_tweet_image_ids/'+val_data.df.iloc[idx]['filename']).convert('RGB')   \n",
    "display(raw_image.resize((596, 437)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150de95a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
